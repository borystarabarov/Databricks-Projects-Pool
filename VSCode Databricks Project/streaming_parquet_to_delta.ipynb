{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Parquet Files to Delta Table\n",
    "This notebook reads Parquet files as a stream and writes them to a Delta table with schema evolution enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure spark to handle schema evolution\n",
    "spark.conf.set(\"spark.sql.streaming.schemaInference\", True)\n",
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the streaming parquet files\n",
    "stream_df = spark.readStream.format(\"parquet\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .load(\"abfss://external-location@databricksdevstgacc.dfs.core.windows.net/files2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the stream to delta table\n",
    "query = stream_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"abfss://external-location@databricksdevstgacc.dfs.core.windows.net/checkpoints/stream_test\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .table(\"dev_catalog.default.stream_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the streaming query to terminate\n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
