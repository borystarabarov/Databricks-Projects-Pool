{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream Parquet Files to Delta Table\n",
    "This notebook reads Parquet files as a stream and writes them to a Delta table with schema handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure spark to handle schema merging\n",
    "spark.conf.set(\"spark.sql.streaming.schemaInference\", True)\n",
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input and output paths\n",
    "input_path = \"abfss://external-location@databricksdevstgacc.dfs.core.windows.net/files2/\"\n",
    "output_table = \"dev_catalog.default.stream_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the streaming data\n",
    "stream_df = spark.readStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .load(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the streaming data to Delta table\n",
    "stream_query = stream_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", \"_checkpoint/stream_test\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(processingTime='1 minute') \\\n",
    "    .table(output_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the streaming query to finish\n",
    "stream_query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
